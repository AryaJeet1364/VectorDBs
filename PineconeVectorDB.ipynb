{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPyU+MFqahd3n3PCM79w+1f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryaJeet1364/VectorDBs/blob/main/PineconeVectorDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pinecone Hands-on"
      ],
      "metadata": {
        "id": "dxXN_-PaFtwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I built a minimal Retrieval-Augmented Generation (RAG) pipeline that uses Pinecone as a vector database, SentenceTransformers to create semantic embeddings of documents, and FLAN-T5 for generating context-aware answers to user queries. The system takes a query, retrieves the most relevant text chunks based on vector similarity, and generates grounded answers using the retrieved context â€” all without relying on LangChain or external APIs, keeping the code lightweight and easy to understand."
      ],
      "metadata": {
        "id": "9agZoC_xF4xb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies"
      ],
      "metadata": {
        "id": "gPM0gpAjHzk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"pinecone-client[grpc]\" sentence-transformers transformers torch PyPDF2 accelerate"
      ],
      "metadata": {
        "id": "tz-JEoS2juG-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y pinecone-client\n",
        "!pip install -q pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41VY2CCyj9Zr",
        "outputId": "c7b02abe-3c87-40d5-bdfc-4718e2f3de48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pinecone-client 6.0.0\n",
            "Uninstalling pinecone-client-6.0.0:\n",
            "  Successfully uninstalled pinecone-client-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "LlTZ1khpjvl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import PyPDF2\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "3BWewTA2CFD_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "EEkP9kf4CKm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd1c8a1-9426-4073-fec8-e2b4f913ee6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Pinecone API Key"
      ],
      "metadata": {
        "id": "3vcxS6DXIWKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "PINECONE_API_ENV = userdata.get('PINECONE_API_ENV')\n",
        "\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "INDEX_NAME = \"rag-documents\"\n",
        "DIMENSION = 384\n",
        "\n",
        "try:\n",
        "    if INDEX_NAME not in pc.list_indexes().names():\n",
        "        pc.create_index(\n",
        "            name=INDEX_NAME,\n",
        "            dimension=DIMENSION,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=PINECONE_API_ENV)\n",
        "        )\n",
        "        print(f\"Created index: {INDEX_NAME}\")\n",
        "    else:\n",
        "        print(f\"Index '{INDEX_NAME}' already exists\")\n",
        "\n",
        "    index = pc.Index(INDEX_NAME)\n",
        "    print(\"Connected to Pinecone\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Pinecone setup failed:\", e)\n",
        "\n"
      ],
      "metadata": {
        "id": "tDJv9QP0H2uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b596ea7-5016-4c3f-9e55-b8bd75a65cdd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'rag-documents' already exists\n",
            "Connected to Pinecone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Model"
      ],
      "metadata": {
        "id": "9StmA4NjdgOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').to(device)\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "text_generator = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    max_length=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo9o0mrqdjba",
        "outputId": "76ee7842-ea50-45d8-fd3c-50ac1ad78e32"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Processing"
      ],
      "metadata": {
        "id": "aicEQCWSez1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=500, chunk_overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - chunk_overlap):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "def upload_sample_text():\n",
        "    return [\n",
        "        \"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines. AI can learn and solve problems.\",\n",
        "        \"Retrieval-Augmented Generation (RAG) combines document search with text generation for more accurate responses.\",\n",
        "        \"Vector databases like Pinecone store embeddings and allow fast semantic search.\",\n",
        "        \"Natural Language Processing (NLP) is about making computers understand and generate human language.\",\n",
        "        \"Large Language Models (LLMs) like GPT are trained on huge datasets and can generate and understand language.\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "AdHRYeLCe2P9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings and Indexing"
      ],
      "metadata": {
        "id": "LdUpriLEe8HN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(texts):\n",
        "    return embedding_model.encode(texts, convert_to_tensor=True).cpu().numpy()\n",
        "\n",
        "def index_documents(texts, metadata=None):\n",
        "    if metadata is None:\n",
        "        metadata = [{\"text\": text, \"source\": f\"doc_{i}\"} for i, text in enumerate(texts)]\n",
        "\n",
        "    embeddings = generate_embeddings(texts)\n",
        "\n",
        "    vectors = [\n",
        "        {\"id\": f\"doc_{i}\", \"values\": emb.tolist(), \"metadata\": meta}\n",
        "        for i, (emb, meta) in enumerate(zip(embeddings, metadata))\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        index.upsert(vectors=vectors)\n",
        "        print(f\"Indexed {len(vectors)} documents\")\n",
        "    except Exception as e:\n",
        "        print(\"Indexing failed:\", e)\n"
      ],
      "metadata": {
        "id": "HBcI8LfGe_fE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval and Generation\n"
      ],
      "metadata": {
        "id": "P5dAgE17fDdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_documents(query, top_k=3):\n",
        "    query_vec = generate_embeddings([query])[0]\n",
        "    try:\n",
        "        results = index.query(vector=query_vec.tolist(), top_k=top_k, include_metadata=True)\n",
        "        return results['matches']\n",
        "    except Exception as e:\n",
        "        print(\"Search error:\", e)\n",
        "        return []\n",
        "\n",
        "def generate_answer(query, docs):\n",
        "    context = \"\\n\\n\".join([doc['metadata']['text'] for doc in docs])\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer based on the context above:\"\n",
        "    try:\n",
        "        return text_generator(prompt, max_length=200)[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        print(\"Answer generation failed:\", e)\n",
        "        return \"No answer generated.\"\n",
        "\n",
        "def rag_pipeline(query):\n",
        "    print(f\"\\nQuery: {query}\\n{'-'*50}\")\n",
        "    docs = search_documents(query)\n",
        "    if not docs:\n",
        "        return {\"query\": query, \"answer\": \"No relevant documents found.\", \"sources\": []}\n",
        "\n",
        "    answer = generate_answer(query, docs)\n",
        "    sources = [{\n",
        "        \"rank\": i + 1,\n",
        "        \"score\": doc['score'],\n",
        "        \"text\": doc['metadata']['text'][:200] + \"...\",\n",
        "        \"source\": doc['metadata'].get('source', 'Unknown')\n",
        "    } for i, doc in enumerate(docs)]\n",
        "\n",
        "    return {\"query\": query, \"answer\": answer, \"sources\": sources}"
      ],
      "metadata": {
        "id": "Tg0WpZiifIru"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Func"
      ],
      "metadata": {
        "id": "E_JuY3VVfMv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    sample_texts = upload_sample_text()\n",
        "    index_documents(sample_texts)\n",
        "\n",
        "    demo_queries = [\n",
        "        \"What is AI?\",\n",
        "        \"What is RAG?\",\n",
        "        \"How do vector databases help?\",\n",
        "        \"Explain NLP\",\n",
        "        \"What are LLMs?\"\n",
        "    ]\n",
        "\n",
        "    for q in demo_queries:\n",
        "        res = rag_pipeline(q)\n",
        "        print(f\"Answer: {res['answer']}\")\n",
        "        print(f\"Sources: {len(res['sources'])}\")\n",
        "        print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "qTsD-8SufOnV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlmCn4jpfV9H",
        "outputId": "1654f152-76dc-4e78-fb1a-9fa5f01f1fb7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexed 5 documents\n",
            "\n",
            "Query: What is AI?\n",
            "--------------------------------------------------\n",
            "Answer: simulation of human intelligence processes by machines\n",
            "Sources: 3\n",
            "----------------------------------------\n",
            "\n",
            "Query: What is RAG?\n",
            "--------------------------------------------------\n",
            "Answer: combines document search with text generation for more accurate responses\n",
            "Sources: 3\n",
            "----------------------------------------\n",
            "\n",
            "Query: How do vector databases help?\n",
            "--------------------------------------------------\n",
            "Answer: store embeddings and allow fast semantic search\n",
            "Sources: 3\n",
            "----------------------------------------\n",
            "\n",
            "Query: Explain NLP\n",
            "--------------------------------------------------\n",
            "Answer: The relevant sentence in the passage is: Natural Language Processing (NLP) is about making computers understand and generate human language. The relevant sentence in the passage is: Large Language Models (LLMs) like GPT are trained on huge datasets and can generate and understand language. The relevant sentence in the passage is: Retrieval-Augmented Generation (RAG) combines document search with text generation for more accurate responses. The relevant sentence in the passage is: Natural Language Processing (NLP) is about making computers understand and generate human language. The relevant sentence in the passage is: Large Language Models (LLMs) like GPT are trained on huge datasets and can generate and understand language. The relevant sentence in the passage is: Retrieval-Augmented Generation (RAG) combines document search with text generation for more accurate responses.\n",
            "Sources: 3\n",
            "----------------------------------------\n",
            "\n",
            "Query: What are LLMs?\n",
            "--------------------------------------------------\n",
            "Answer: Large Language Models\n",
            "Sources: 3\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLuJbU9CkXT3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}