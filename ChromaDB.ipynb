{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6BibmLDmW18feaz2T20OO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryaJeet1364/VectorDBs/blob/main/ChromaDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChromaDB Hands-on -- Intro to Vector Databases"
      ],
      "metadata": {
        "id": "Ow_VeUoD2z1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I learned how to build a complete local document-based Question Answering (QA) system using LangChain, ChromaDB, and a free HuggingFace model — all without needing any paid API keys. I understood how to load and preprocess raw text data, convert it into vector embeddings using sentence-transformers, store and retrieve relevant chunks using ChromaDB, and finally use a local LLM (flan-t5-base) to answer natural language queries based on the documents. I also gained hands-on experience in building retrieval-augmented generation (RAG) pipelines and making them fully reproducible, efficient, and privacy-friendly."
      ],
      "metadata": {
        "id": "NOsTECp5AiUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "jM9g_o8GAOkt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eexIdPUxyn45"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-community chromadb sentence-transformers tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAF9hWCq2_VQ",
        "outputId": "53076586-53a5-4c25-a544-bd06f39b8e54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: chromadb\n",
            "Version: 1.0.12\n",
            "Summary: Chroma.\n",
            "Home-page: https://github.com/chroma-core/chroma\n",
            "Author: \n",
            "Author-email: Jeff Huber <jeff@trychroma.com>, Anton Troynikov <anton@trychroma.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: bcrypt, build, fastapi, grpcio, httpx, importlib-resources, jsonschema, kubernetes, mmh3, numpy, onnxruntime, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, opentelemetry-sdk, orjson, overrides, posthog, pydantic, pypika, pyyaml, rich, tenacity, tokenizers, tqdm, typer, typing-extensions, uvicorn\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "OIPIvBkgALlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sample data"
      ],
      "metadata": {
        "id": "iTmWcFNc3S8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip"
      ],
      "metadata": {
        "id": "uzzk_Gae3FHU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q new_articles.zip -d new_articles"
      ],
      "metadata": {
        "id": "s3dg16DL3YGI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Text Files"
      ],
      "metadata": {
        "id": "5YcFvNdb5Nmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "\n",
        "loader = DirectoryLoader(\"new_articles\", glob=\"*.txt\", loader_cls=TextLoader)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "qtUFyJTw5Lxb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Vector DB"
      ],
      "metadata": {
        "id": "atV6dhvD-_z8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting Text"
      ],
      "metadata": {
        "id": "2rOXuhjL8Ypa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "bo0tbnSN5Tda"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Model -- Free"
      ],
      "metadata": {
        "id": "ux2r__nI8eJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYbbHnvh8dyU",
        "outputId": "4b9f66e1-19c6-452f-c350-f26788c4427a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-1883683424>:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector Store : ChromaDB"
      ],
      "metadata": {
        "id": "6g0DIXwH8nd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "persist_directory = \"db\"\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=texts,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "vectordb.persist()\n",
        "vectordb = None  # Free memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhT-ltH38bZJ",
        "outputId": "77797458-84ca-4188-ff63-cc394a70c4a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-161298657>:11: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectordb.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making a Retriever"
      ],
      "metadata": {
        "id": "xLIESOa18s30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=embedding\n",
        ")\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b1lkdt28q19",
        "outputId": "c20e2cb8-962c-44aa-b6d3-b4e71d52b0d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-783083852>:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectordb = Chroma(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making a Chain"
      ],
      "metadata": {
        "id": "sqQHY3-X_jdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM"
      ],
      "metadata": {
        "id": "ZOVxSEC_9AQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model locally (no API key needed)\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a free & public model like flan-t5-base\n",
        "local_llm_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=256)\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=local_llm_pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCUlo-se8yWy",
        "outputId": "7256d305-e475-42ea-cc89-34a05e7d7047"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "<ipython-input-10-80465619>:8: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=local_llm_pipeline)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval QA"
      ],
      "metadata": {
        "id": "VUWZ5mBC9lCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,  # from ChromaDB setup\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# Ask a question\n",
        "query = \"How much money did Microsoft raise?\"\n",
        "response = qa_chain(query)\n",
        "\n",
        "# Print nicely\n",
        "def process_llm_response(llm_response):\n",
        "    print(\"Answer:\", llm_response['result'])\n",
        "    print(\"\\nSources:\")\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(\"-\", source.metadata['source'])\n",
        "\n",
        "process_llm_response(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTNi4ttm9Bb0",
        "outputId": "49c44bfb-1336-4c3e-f0f2-237159d0f781"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-2778623849>:12: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa_chain(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: VC firms including Sequoia Capital, Andreessen Horowitz, Thrive and K2 Global are picking up new shares, according to documents seen by TechCrunch. A source tells us Founders Fund is also investing. Altogether the VCs have put in just over $300 million at a valuation of $27 billion to $29 billion. This is separate to a big investment from Microsoft announced earlier this year, a person familiar with the development told TechCrunch, which closed in January. The size of Microsoft’s investment is believed to be around $10 billion, a figure we confirmed with our source. April 25, 2023 Called ChatGPT Business, OpenAI describes the forthcoming offering as “for professionals who need more control over their data as well as enterprises seeking to manage their end users.”\n",
            "\n",
            "Sources:\n",
            "- new_articles/05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt\n",
            "- new_articles/05-04-microsoft-doubles-down-on-ai-with-new-bing-features.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick Test"
      ],
      "metadata": {
        "id": "M4EMsM7K_3qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"What is the capital of France?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "SPWIACPd9ng-",
        "outputId": "ecb86c9d-79d7-48c2-9146-54360e785eae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-3143015057>:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  llm(\"What is the capital of France?\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'london'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deleting the DB"
      ],
      "metadata": {
        "id": "QCdEwauVACsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb.delete_collection()\n",
        "vectordb.persist()\n",
        "\n",
        "!rm -rf db/"
      ],
      "metadata": {
        "id": "-Nb1C6n59udO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZEdKRCAmASWa"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}